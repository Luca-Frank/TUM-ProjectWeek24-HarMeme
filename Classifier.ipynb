{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data_train_preprocessed.csv\")\n",
    "test_df = pd.read_csv(\"data_test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_text_alt</th>\n",
       "      <th>binary_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>covid_memes_18</td>\n",
       "      <td>covid_memes_18.png</td>\n",
       "      <td>['somewhat harmful', 'individual']</td>\n",
       "      <td>Bernie or Elizabeth?\\nBe informed.Compare them...</td>\n",
       "      <td>bernie elizabeth inform compare issue matter i...</td>\n",
       "      <td>bernie elizabeth issue matter issue make danke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>covid_memes_19</td>\n",
       "      <td>covid_memes_19.png</td>\n",
       "      <td>['somewhat harmful', 'organization']</td>\n",
       "      <td>Extending the\\nBrexit deadline until\\nOctober ...</td>\n",
       "      <td>extend brexit deadline october order ensure de...</td>\n",
       "      <td>extending brexit deadline october 31st order e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>covid_memes_252</td>\n",
       "      <td>covid_memes_252.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>kwai\\ngkwa 0964\\n#nnevvy\\napplause to Thais fr...</td>\n",
       "      <td>kwai gkwa nnevvy applause thais hong kong thai...</td>\n",
       "      <td>kwai gkwa 0964 nnevvy applause thai hong kong ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>covid_memes_255</td>\n",
       "      <td>covid_memes_255.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>So, I order this\\nfoce mask to\\nprotect ogains...</td>\n",
       "      <td>order foce mask protect ogainst fhe corond vir...</td>\n",
       "      <td>order foce mask protect ogainst fhe corond vir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>covid_memes_20</td>\n",
       "      <td>covid_memes_20.png</td>\n",
       "      <td>['somewhat harmful', 'individual']</td>\n",
       "      <td>best candidate for\\nJA\\n2020\\njoe biden\\nKamal...</td>\n",
       "      <td>good candidate ja joe biden kamala harris bern...</td>\n",
       "      <td>best candidate ja 2020 joe biden kamala harris...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>3008</td>\n",
       "      <td>covid_memes_5417</td>\n",
       "      <td>covid_memes_5417.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>Jim Halpert\\n@JimHalpert\\neverybody: 2020 is f...</td>\n",
       "      <td>jim halpert @jimhalpert everybody finally go y...</td>\n",
       "      <td>jim halpert jimhalpert everybody 2020 finally ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>3009</td>\n",
       "      <td>covid_memes_5418</td>\n",
       "      <td>covid_memes_5418.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>litquidity\\nelihcapital\\nyofollewine\\n*covid 1...</td>\n",
       "      <td>litquidity elihcapital yofollewine covid sympt...</td>\n",
       "      <td>litquidity elihcapital yofollewine covid 19 sy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>3010</td>\n",
       "      <td>covid_memes_5419</td>\n",
       "      <td>covid_memes_5419.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>meta\\nMe sending my dog out for supplies since...</td>\n",
       "      <td>meta send dog supply contract covid-19 coc ma ...</td>\n",
       "      <td>meta sending dog supply since contract coc 100...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>3011</td>\n",
       "      <td>covid_memes_5420</td>\n",
       "      <td>covid_memes_5420.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>People born in March/April in the\\nboveteojoe ...</td>\n",
       "      <td>people bear march april boveteojoe folee come ...</td>\n",
       "      <td>people born boveteojoe foleing coming week soy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>3012</td>\n",
       "      <td>covid_memes_5421</td>\n",
       "      <td>covid_memes_5421.png</td>\n",
       "      <td>['not harmful']</td>\n",
       "      <td>Me after washing my hands for 20\\nseconds 57 t...</td>\n",
       "      <td>wash hand second time day</td>\n",
       "      <td>washing hand 20 second 57 time one day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3013 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                id                 image  \\\n",
       "0              0    covid_memes_18    covid_memes_18.png   \n",
       "1              1    covid_memes_19    covid_memes_19.png   \n",
       "2              2   covid_memes_252   covid_memes_252.png   \n",
       "3              3   covid_memes_255   covid_memes_255.png   \n",
       "4              4    covid_memes_20    covid_memes_20.png   \n",
       "...          ...               ...                   ...   \n",
       "3008        3008  covid_memes_5417  covid_memes_5417.png   \n",
       "3009        3009  covid_memes_5418  covid_memes_5418.png   \n",
       "3010        3010  covid_memes_5419  covid_memes_5419.png   \n",
       "3011        3011  covid_memes_5420  covid_memes_5420.png   \n",
       "3012        3012  covid_memes_5421  covid_memes_5421.png   \n",
       "\n",
       "                                    labels  \\\n",
       "0       ['somewhat harmful', 'individual']   \n",
       "1     ['somewhat harmful', 'organization']   \n",
       "2                          ['not harmful']   \n",
       "3                          ['not harmful']   \n",
       "4       ['somewhat harmful', 'individual']   \n",
       "...                                    ...   \n",
       "3008                       ['not harmful']   \n",
       "3009                       ['not harmful']   \n",
       "3010                       ['not harmful']   \n",
       "3011                       ['not harmful']   \n",
       "3012                       ['not harmful']   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Bernie or Elizabeth?\\nBe informed.Compare them...   \n",
       "1     Extending the\\nBrexit deadline until\\nOctober ...   \n",
       "2     kwai\\ngkwa 0964\\n#nnevvy\\napplause to Thais fr...   \n",
       "3     So, I order this\\nfoce mask to\\nprotect ogains...   \n",
       "4     best candidate for\\nJA\\n2020\\njoe biden\\nKamal...   \n",
       "...                                                 ...   \n",
       "3008  Jim Halpert\\n@JimHalpert\\neverybody: 2020 is f...   \n",
       "3009  litquidity\\nelihcapital\\nyofollewine\\n*covid 1...   \n",
       "3010  meta\\nMe sending my dog out for supplies since...   \n",
       "3011  People born in March/April in the\\nboveteojoe ...   \n",
       "3012  Me after washing my hands for 20\\nseconds 57 t...   \n",
       "\n",
       "                                         processed_text  \\\n",
       "0     bernie elizabeth inform compare issue matter i...   \n",
       "1     extend brexit deadline october order ensure de...   \n",
       "2     kwai gkwa nnevvy applause thais hong kong thai...   \n",
       "3     order foce mask protect ogainst fhe corond vir...   \n",
       "4     good candidate ja joe biden kamala harris bern...   \n",
       "...                                                 ...   \n",
       "3008  jim halpert @jimhalpert everybody finally go y...   \n",
       "3009  litquidity elihcapital yofollewine covid sympt...   \n",
       "3010  meta send dog supply contract covid-19 coc ma ...   \n",
       "3011  people bear march april boveteojoe folee come ...   \n",
       "3012                          wash hand second time day   \n",
       "\n",
       "                                     processed_text_alt  binary_labels  \n",
       "0     bernie elizabeth issue matter issue make danke...              0  \n",
       "1     extending brexit deadline october 31st order e...              0  \n",
       "2     kwai gkwa 0964 nnevvy applause thai hong kong ...              0  \n",
       "3     order foce mask protect ogainst fhe corond vir...              0  \n",
       "4     best candidate ja 2020 joe biden kamala harris...              0  \n",
       "...                                                 ...            ...  \n",
       "3008  jim halpert jimhalpert everybody 2020 finally ...              0  \n",
       "3009  litquidity elihcapital yofollewine covid 19 sy...              0  \n",
       "3010  meta sending dog supply since contract coc 100...              0  \n",
       "3011  people born boveteojoe foleing coming week soy...              0  \n",
       "3012             washing hand 20 second 57 time one day              0  \n",
       "\n",
       "[3013 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TF-IDF Vectorization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)  \u001b[38;5;66;03m# You can adjust max_features based on your dataset size\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_text_alt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text_alt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m y_train \u001b[38;5;241m=\u001b[39m (train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_labels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your dataset size\n",
    "X_train = vectorizer.fit_transform(train_df['processed_text_alt'])\n",
    "X_test = vectorizer.transform(test_df['processed_text_alt'])\n",
    "y_train = (train_df['binary_labels'] == 1).astype(int)\n",
    "y_test = (test_df['binary_labels'] == 1).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
